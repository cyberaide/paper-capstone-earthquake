@misc{fox2021earthquake,
  title =	 {Earthquake Nowcasting with Deep Learning},
  author =	 {Geoffrey Fox and John Rundle and Andrea Donnellan
                  and Bo Feng},
  year =	 2021,
  eprint =	 {2201.01869},
  archivePrefix ={arXiv},
  primaryClass = {physics.geo-ph}
}

@InProceedings{fox2022aiforscience,
  author =	 {Geoffrey Fox and John Rundle and Bo Feng},
  title =	 {{AI for Science illustrated by Deep Learning for
                  Geospatial Time Series}},
  year =	 2022,
  month =	 jan,
  organization = {Digital Science Center},
  publisher =	 {IEEE},
  series =	 {IEEE 12th Annual Computing and Communication
                  Workshop and Conference},
  url =
                  {https://docs.google.com/presentation/d/1UiMVV4mMzIXIzBJ2WqFrWnqVI6NaI0jevb7BKulun2c/edit?usp=sharing}
}

@misc{las-intro-python,
  author =	 {von Laszewski, Gregor},
  title =	 {Introduction to Python},
  howpublished = {Online Book},
  year =	 2022,
  month =	 jan,
  url =
                  {https://cloudmesh-community.github.io/pub/vonLaszewski-python.pdf}
}

@misc{vaswani2017attention,
  title =	 {Attention Is All You Need},
  author =	 {Ashish Vaswani and Noam Shazeer and Niki Parmar and
                  Jakob Uszkoreit and Llion Jones and Aidan N. Gomez
                  and Lukasz Kaiser and Illia Polosukhin},
  year =	 2017,
  eprint =	 {1706.03762},
  archivePrefix ={arXiv},
  primaryClass = {cs.CL}
}

@misc{www-aurora,
  title =	 {{Aurora Argonne Leadership Computing Facility}},
  year =	 2022,
  month =	 jan,
  url =		 {https://www.alcf.anl.gov/aurora},
  howpublished = {Web Page}
}

@misc{www-conda,
  title =	 {{Conda}},
  author =	 {{Conda Developers}},
  year =	 2022,
  month =	 jan,
  url =		 {https://docs.conda.io/en/latest},
  howpublished = {Web Page}
}

@misc{www-dgx-station-a100,
  title =	 {{NVIDIA DGX Station A100}},
  author =	 {{NVIDIA}},
  year =	 2022,
  month =	 jan,
  url =
                  {https://www.nvidia.com/en-us/data-center/dgx-station-a100},
  year =	 2022,
  month =	 jan,
  howpublished = {Web Page}
}

@misc{www-expanse,
  title =	 {{Expanse}},
  year =	 2022,
  month =	 jan,
  url =
                  {https://www.sdsc.edu/services/hpc/expanse/index.html},
  howpublished = {Web Page}
}

@Misc{www-horovod,
  author =	 {{Horovod Developers}},
  howpublished = {Web Page},
  month =	 jan,
  title =	 {{Horovod}},
  year =	 2022,
  abstract =	 {Horovod is a distributed deep learning training
                  framework for TensorFlow, Keras, PyTorch, and Apache
                  MXNet. The goal of Horovod is to make distributed
                  deep learning fast and easy to use.},
  journal =	 {GitHub},
  url =		 {https://github.com/horovod/horovod},
}

@Misc{www-jabrefg-org,
  author =	 {{jabref.org}},
  howpublished = {Web Page},
  month =	 jan,
  title =	 {{JabRef}},
  year =	 2022,
  url =		 {https://jabref.org},
}

@misc{www-jupyterlab,
  author =	 {{Project Jupyter}},
  title =	 {Jupyterlab (version ?)},
  year =	 2022,
  url =		 {https://jupyter.org},
  howpublished = {Web Page}
}

@misc{www-mlcommons,
  author =	 {{MLCommons}},
  title =	 {MLCommons},
  year =	 2022,
  url =		 {https://mlcommons.org/},
  howpublished = {Web Page}
}

@Misc{www-mlcommons-eathquake,
  author =	 {Geoffery Fox and Gregor von Laszewski},
  howpublished = {Web Page},
  title =	 {{MLCommons Earthquake Science Benchmark}},
  year =	 2013,
  journal =	 {GitHub Repository},
  publisher =	 {GitHub},
  url =
                  {https://github.com/Data-ScienceHub/mlcommons-science},
}

@misc{www-mlcube,
  author =	 {{MLCommons}},
  title =	 {MLCube},
  howpublished = {Web Page},
  year =	 2022,
  month =	 jan,
  url =		 {https://mlcommons.org/en/mlcube/},
  abstract =	 {MLCube is a set of common conventions for creating
                  ML software that can "plug-and-play" on many
                  different systems. MLCube makes it easier for
                  researchers to share innovative ML models, for a
                  developer to experiment with many different models,
                  and for software companies to create infrastructure
                  for models. It creates opportunities by putting ML
                  in the hands of more people.},
}

@Misc{www-modules,
  author =	 {{Lmod Developers}},
  howpublished = {Web Page},
  month =	 jan,
  title =	 {{Lmod: A New Environment Module System}},
  year =	 2022,
  url =		 {https://lmod.readthedocs.io/en/latest},
}

@Misc{www-onnen2021,
  author =	 {Onnen, Heiko},
  howpublished = {Blog},
  month =	 dec,
  title =	 {{Temporal Fusion Transformer: A Primer on deep
                  forecasting in python}},
  year =	 2021,
  publisher =	 {Towards Data Science},
  url =
                  {https://towardsdatascience.com/temporal-fusion-transformer-a-primer-on-deep-forecasting-in-python-4eb37f3f3594},
}

@misc{www-papermill,
  title =	 {{Papermill 2.3.4 documentation}},
  author =	 {{Papermill Developers}},
  year =	 2022,
  month =	 jan,
  url =		 {https://papermill.readthedocs.io/en/latest},
  howbublished = {Web Page}
}

@misc{www-pearl-1,
  title =	 {{SCD Introducing the small, yet powerful, PEARL
                  system for AI and Machine Learning}},
  year =	 2022,
  month =	 jan,
  author =	 {{Rutherford Appleton Laboratory}},
  url =
                  {https://www.scd.stfc.ac.uk/Pages/Introducing-the-PEARL-system-for-AI-and-Machine-Learning.aspx},
  howpublished = {Web Page}
}

@misc{www-perlmutter,
  title =	 {{Perlmutter}},
  journal =	 {NERSC},
  url =		 {https://www.nersc.gov/systems/perlmutter},
  year =	 2022,
  month =	 jan,
  howpublished = {Web Page}
}

@misc{www-pytorch,
  title =	 {{PyTorch}},
  year =	 2022,
  month =	 jan,
  url =		 {https://pytorch.org},
  howpublished = {Web Page}
}

@InProceedings{pytorch-paper,
  author =	 {Paszke, Adam and Gross, Sam and Massa, Francisco and
                  Lerer, Adam and Bradbury, James and Chanan, Gregory
                  and Killeen, Trevor and Lin, Zeming and Gimelshein,
                  Natalia and Antiga, Luca and Desmaison, Alban and
                  Kopf, Andreas and Yang, Edward and DeVito, Zachary
                  and Raison, Martin and Tejani, Alykhan and
                  Chilamkurthy, Sasank and Steiner, Benoit and Fang,
                  Lu and Bai, Junjie and Chintala, Soumith},
  title =	 {PyTorch: An Imperative Style, High-Performance Deep
                  Learning Library},
  booktitle =	 {33rd Conference on Neural Information Processing
                  Systems (NeurIPS 2019)},
  editor =	 {H. Wallach and H. Larochelle and A. Beygelzimer and
                  F. d\textquotesingle Alch\'{e}-Buc and E. Fox and
                  R. Garnett},
  address =	 {Vancouver, Canada},
  publisher =	 {TODO: ??? Curran Associates, Inc.},
  abstract =	 {Deep learning frameworks have often focused on
                  either usability or speed, but not both. PyTorch is
                  a machine learning library that shows that these two
                  goals are in fact compatible: it provides an
                  imperative and Pythonic programming style that
                  supports code as a model, makes debugging easy and
                  is consistent with other popular scientific
                  computing libraries, while remaining efficient and
                  supporting hardware accelerators such as GPUs.  In
                  this paper, we detail the principles that drove the
                  implementation of PyTorch and how they are reflected
                  in its architecture. We emphasize that every aspect
                  of PyTorch is a regular Python program under the
                  full control of its user. We also explain how the
                  careful and pragmatic implementation of the key
                  components of its runtime enables them to work
                  together to achieve compelling performance. We
                  demonstrate the efficiency of individual subsystems,
                  as well as the overall speed of PyTorch on several
                  common benchmarks.},
  url =
                  {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  year =	 2019,
  month =	 jan,
}

@misc{www-rivanna,
  title =	 {{Rivanna at University of Virginia Research
                  Computing}},
  year =	 2022,
  month =	 jan,
  url =
                  {https://www.rc.virginia.edu/userinfo/rivanna/overview},
  howpublished = {Web Page}
}

@misc{www-summit,
  title =	 {{Summit}},
  journal =	 {Oak Ridge Leadership Computing Facility},
  year =	 2022,
  month =	 jan,
  url =		 {https://www.olcf.ornl.gov/summit},
  howpublished = {Web Page}
}
