@misc{www-expanse,
	title = {{Expanse}},
	year = 2022,
	month = jan,
	url = {https://www.sdsc.edu/services/hpc/expanse/index.html},
    howpublished={Web Page}
}

@misc{www-aurora,
	title = {{Aurora Argonne Leadership Computing Facility}},
	year = 2022,
	month = jan,
	url = {https://www.alcf.anl.gov/aurora},
    howpublished={Web Page}
}

@misc{www-rivanna,
	title = {{Rivanna at University of Virginia Research Computing}},
	year = 2022,
	month = jan,
    url = {https://www.rc.virginia.edu/userinfo/rivanna/overview},
    howpublished={Web Page}
}

@misc{www-pearl-1,
	title = {{SCD Introducing the small, yet powerful, PEARL system for AI and Machine Learning}},
	year = 2022,
	month = jan,
	author = {{Rutherford Appleton Laboratory}},
	url = {https://www.scd.stfc.ac.uk/Pages/Introducing-the-PEARL-system-for-AI-and-Machine-Learning.aspx},
	howpublished = {Web Page}
}

@misc{www-perlmutter,
	title = {{Perlmutter}},
	journal = {NERSC},
	url = {https://www.nersc.gov/systems/perlmutter},
	year = 2022,
	month = jan,
    howpublished={Web Page}
}

@misc{www-summit,
	title = {{Summit}},
	journal = {Oak Ridge Leadership Computing Facility},
	year = 2022,
	month = jan,
    url = {https://www.olcf.ornl.gov/summit},
    howpublished={Web Page}
}

@misc{www-dgx-station-a100,
	title = {{NVIDIA DGX Station A100}},
	author = {{NVIDIA}},
	year = 2022,
	month = jan,
	url = {https://www.nvidia.com/en-us/data-center/dgx-station-a100},
	year = 2022,
	month = jan,
    howpublished={Web Page}
}

@misc{www-mlcube,
	title = {{MLCube{\texttrademark}}},
	author = {{MLCommons}},
	year = 2022,
	month = jan,
	url = {https://mlcommons.org/en/mlcube},
    howpublished={Web Page}
}

@misc{www-horovod,
	author = {{horovod developers}},
	title = {{Horovod}},
	journal = {GitHub},
	year = 2022,
	month = jan,
	url = {https://github.com/horovod/horovod},
    howpublished={Web Page}
}

@misc{www-pytorch,
	title = {{PyTorch}},
	year = 2022,
	month = jan,
	url = {https://pytorch.org},
    howpublished={Web Page}
}

@misc{www-conda,
	title = {{Conda}},
	author= {{Conda Developers}},
	year = 2022,
	month = jan,
	url = {https://docs.conda.io/en/latest},
    howpublished={Web Page}
}

@misc{www-modules,
	title = {{Lmod: A New Environment Module System}},
	year = 2022,
	month = jan,
	url = {https://lmod.readthedocs.io/en/latest},
    howpublished={Web Page}
}

@misc{www-papermill,
	title = {{Papermill 2.3.4 documentation}},
	author={{Papermill Developers}},
	year = 2022,
	month = jan,
	url = {https://papermill.readthedocs.io/en/latest},
	howbublished = {Web Page}
}

@misc{www-jabrefg-org,
    author={{jabref.org}},
    title={JabRef},
    howpublished={Web Page},
    year = 2022,
    month = jan,
    url={https://jabref.org}
}
@misc{las-intro-python,
    author={von Laszewski, Gregor},
    title={Introduction to Python},
    howpublished={Online Book},
    year = 2020,
    month = Aug,
    url={https://cloudmesh-community.github.io/pub/vonLaszewski-python.pdf}
}

@misc{fox2021earthquake,
      title={Earthquake Nowcasting with Deep Learning}, 
      author={Geoffrey Fox and John Rundle and Andrea Donnellan and Bo Feng},
      year={2021},
      eprint={2201.01869},
      archivePrefix={arXiv},
      primaryClass={physics.geo-ph}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{www-mlcommons-eathquake,
  author = {Geoffery Fox and Gregor von Laszewski},
  title = {MLCommons Earthquate Science Benchmark},
  year = {2013},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/cybertraining-dsc/capstone-eartquake}},
  commit = {}
}

 @misc{www-onnen2021,
   title={Temporal Fusion Transformer: A Primer on deep forecasting in python},
   url ={https://towardsdatascience.com/temporal-fusion-transformer-a-primer-on-deep-forecasting-in-python-4eb37f3f3594},
   howpublished={Blog},
   publisher={Towards Data Science},
   author={Onnen, Heiko},
   year={2021},
   month={Dec}
 } 

 @misc{www-mlcommons-mlcube,
    author={{mlcommons.org}},
    title={MLCube},
    howpublished={Web Page},
    year = 2022,
    month = jan,
    url={https://mlcommons.org/en/mlcube/}
}

@InProceedings{fox2022aiforscience,
  author       = {Geoffrey Fox and John Rundle and Bo Feng},
  title        = {AI for Science illustrated by Deep Learning for                   Geospatial Time Series},
  year         = 2022,
  series       = {IEEE 12th Annual Computing and Communication Workshop and Conference},
  month        = jan,
  address      = {},
  organization = {Digital Science Center},
  publisher    = {IEEE},
  url = {https://docs.google.com/presentation/d/1UiMVV4mMzIXIzBJ2WqFrWnqVI6NaI0jevb7BKulun2c/edit?usp=sharing}
}

@misc{www-jupyterlab,
  author = {{Project Jupyter}},
  title = {Jupyterlab (version ?)},
  year  = {2022},
  url   = {https://jupyter.org},
  howpublished = {Web Page}
}

@misc{www-mlcommons,
  author = {{MLCommons}},
  title = {MLCommons},
  year = {2022},
  url = {https://mlcommons.org/},
  howpublished = {Web Page}
}

@misc{www-dgx-station-a100,
	title = {{NVIDIA DGX Station A100}},
	author = {{NVIDIA}},
	year = 2022,
	month = jan,
	url = {https://www.nvidia.com/en-us/data-center/dgx-station-a100},
	year = 2022,
	month = jan,
    howpublished={Web Page}
}

@Misc{www-mlcube,
  author       = {{MLCommons}},
  howpublished = {Web Page},
  month        = jan,
  title        = {{MLCube{\texttrademark}}},
  year         = {2022},
  abstract     = {MLCube is a set of common conventions for creating ML software that can "plug-and-play" on many different systems. MLCube makes it easier for researchers to share innovative ML models, for a developer to experiment with many different models, and for software companies to create infrastructure for models. It creates opportunities by putting ML in the hands of more people.},
  url          = {https://mlcommons.org/en/mlcube},
}

@Misc{www-horovod,
  author       = {{Horovod Developers}},
  howpublished = {Web Page},
  month        = jan,
  title        = {{Horovod}},
  year         = {2022},
  abstract     = {Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. The goal of Horovod is to make distributed deep learning fast and easy to use.},
  journal      = {GitHub},
  url          = {https://github.com/horovod/horovod},
}

@Book{www-pytorch,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year      = {2019},
  month     = jan,
  abstract  = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.

  In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
}

@Misc{www-conda,
  author       = {{Conda Developers}},
  howpublished = {Web Page},
  month        = jan,
  title        = {Conda},
  year         = {2022},
  url          = {https://docs.conda.io/en/latest},
}

@Misc{www-modules,
  author       = {{Lmod Developers}},
  howpublished = {Web Page},
  month        = jan,
  title        = {{Lmod: A New Environment Module System}},
  year         = {2022},
  url          = {https://lmod.readthedocs.io/en/latest},
}

@misc{www-papermill,
	title = {{Papermill 2.3.4 documentation}},
	author={{Papermill Developers}},
	year = 2022,
	month = jan,
	url = {https://papermill.readthedocs.io/en/latest},
	howbublished = {Web Page}
}

@Misc{www-jabrefg-org,
  author       = {{jabref.org}},
  howpublished = {Web Page},
  month        = jan,
  title        = {{JabRef}},
  year         = {2022},
  url          = {https://jabref.org},
}

@Misc{las-intro-python,
  author       = {von Laszewski, Gregor},
  howpublished = {Online Book},
  month        = Aug,
  title        = {{Introduction to Python}},
  year         = {2020},
  url          = {https://cloudmesh-community.github.io/pub/vonLaszewski-python.pdf},
}

@Misc{fox2021earthquake,
  author        = {Geoffrey Fox and John Rundle and Andrea Donnellan and Bo Feng},
  title         = {{Earthquake Nowcasting with Deep Learning}},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2201.01869},
  primaryclass  = {physics.geo-ph},
}

@Misc{vaswani2017attention,
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title         = {{Attention Is All You Need}},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1706.03762},
  primaryclass  = {cs.CL},
}

@Misc{www-mlcommons-eathquake,
  author       = {Geoffery Fox and Gregor von Laszewski},
  howpublished = {Web Page},
  title        = {{MLCommons Earthquake Science Benchmark}},
  year         = {2013},
  journal      = {GitHub Repository},
  publisher    = {GitHub},
  url          = {https://github.com/Data-ScienceHub/mlcommons-science},
}

 
@Misc{www-onnen2021,
  author       = {Onnen, Heiko},
  howpublished = {Blog},
  month        = {Dec},
  title        = {{Temporal Fusion Transformer: A Primer on deep forecasting in python}},
  year         = {2021},
  publisher    = {Towards Data Science},
  url          = {https://towardsdatascience.com/temporal-fusion-transformer-a-primer-on-deep-forecasting-in-python-4eb37f3f3594},
}

@InProceedings{fox2022aiforscience,
  author       = {Geoffrey Fox and John Rundle and Bo Feng},
  title        = {{AI for Science illustrated by Deep Learning for Geospatial Time Series}},
  year         = {2022},
  month        = jan,
  organization = {Digital Science Center},
  publisher    = {IEEE},
  series       = {IEEE 12th Annual Computing and Communication Workshop and Conference},
  url          = {https://docs.google.com/presentation/d/1UiMVV4mMzIXIzBJ2WqFrWnqVI6NaI0jevb7BKulun2c/edit?usp=sharing},
}

@misc{www-jupyterlab,
  author = {{Project Jupyter}},
  title = {Jupyterlab (version ?)},
  year  = {2022},
  url   = {https://jupyter.org},
  howpublished = {Web Page}
}

@misc{www-mlcommons,
  author = {{MLCommons}},
  title = {MLCommons},
  year = {2022},
  url = {https://mlcommons.org/},
  howpublished = {Web Page}
}

git@github.com:cyberaide/paper-capstone-mlcommons.git